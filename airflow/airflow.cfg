[core]
dags_folder = /opt/airflow/dags
hostname_callable = airflow.utils.net.getfqdn
default_timezone = utc
executor = SequentialExecutor
parallelism = 32
max_active_tasks_per_dag = 16
dags_are_paused_at_creation = True
max_active_runs_per_dag = 16
task_runner = StandardTaskRunner
fernet_key = Ic-eMp8b_PL-rZi3DZPCIxyVYV9zQ0G_-ne8_VJVu0E=
donot_pickle = True
load_examples = False
plugins_folder = /opt/airflow/plugins
lazy_load_plugins = True
lazy_discover_providers = True
hide_sensitive_var_conn_fields = True
default_task_retries = 0
default_task_retry_delay = 300
enable_xcom_pickling = True

[database]
sql_alchemy_conn = sqlite:////home/airflow/airflow.db
sql_alchemy_pool_enabled = False  # Disable pooling for SQLite
sql_alchemy_max_overflow = 0      # Not needed for SQLite
sql_alchemy_pool_size = 1         # Only 1 connection at a time is sufficient
sql_alchemy_pool_recycle = 1800   # Keep it minimal
sql_alchemy_pool_pre_ping = True  # This is optional but helps keep connections alive

[logging]
base_log_folder = /opt/airflow/logs
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/attempt={{ try_number }}.log
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager.log
processor_manager_log_location = /opt/airflow/logs/processor_manager.log
remote_logging = False
logging_level = INFO

[webserver]
web_server_host = 0.0.0.0
web_server_port = 8080
session_backend = database
workers = 4
secret_key = ${SECRET_KEY}

[scheduler]
job_heartbeat_sec = 10
scheduler_heartbeat_sec = 10
scheduler_idle_sleep_time = 1
min_file_process_interval = 30
dag_dir_list_interval = 300
parsing_processes = 2
max_tis_per_query = 16
use_row_level_locking = True
scheduler_zombie_task_threshold = 300
catchup_by_default = True

[api]
auth_backends = airflow.api.auth.backend.session

[secrets]
backend = airflow.secrets.environment_variables.EnvironmentVariablesBackend

